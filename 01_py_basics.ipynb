{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_py_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python repository for general data manipulation techniques and working with pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will cover:\n",
    "\n",
    "**1. Selecting columns**\n",
    "* basic selection and drop columns\n",
    "* select using pattern recognition\n",
    "* relocate columns\n",
    "\n",
    "**2. Creating new columns and values**\n",
    "* basics of creating new columns\n",
    "* basic sums and numerical column manipulation\n",
    "* row sums and combine columns together\n",
    "* conditionally create columns (ifelse / case_when equivlient)\n",
    "\n",
    "**3. Filtering data**\n",
    "* basic filtering\n",
    "* filter NAs\n",
    "* filter using arrays\n",
    "\n",
    "**4. Aggregations using groupby**\n",
    "* basic groupby aggregation\n",
    "* multiple calculations\n",
    "* groupby with conditional calculations\n",
    "* unnest concatonated cells\n",
    "\n",
    "**5. Joins (merge)**\n",
    "* left join using merge\n",
    "* concatonate dfs together\n",
    "\n",
    "**6. Other operators**\n",
    "* remove duplicates\n",
    "* sort data\n",
    "\n",
    "**Glossary**\n",
    "Glossary of functions used throughout notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic set up to load and inspect data before any data exploration and analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load in basic Python libraris and the data.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note the trade data is not real-world values rather dummy data for the purposes of demonstrations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy are universally used in python, like tidyverse is in R. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip install openpyxl\n",
    "\n",
    "# chnage from scientific notation \n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "trade = pd.read_excel(\"data/trade_data.xlsx\") # upload xlsxl\n",
    "tariff = pd.read_excel(\"data/tariff_data.xlsx\")\n",
    "uk_trqs = pd.read_csv(\"data/uk_trqs.csv\",dtype={'quota__order_number': str})\n",
    "# upload csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic df exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names and types:\n",
    "trade.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df summary:\n",
    "trade.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using .info is very useful as in additional to Dtypes being printed you are provided with the \"non-null\" values or in other words NAs. For example the supression notes column is only NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise numerical values\n",
    "trade.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple df dimensions use shape:\n",
    "trade.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** that the year column is uploaded as a value. It may be preferable to work with a character type rather than value for this column. When uploading data the data type can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = pd.read_excel(\"data/trade_data.xlsx\",dtype={'Year': str}) # convert year to string when uploading data\n",
    "trade3 = pd.read_excel(\"data/trade_data.xlsx\",dtype=str) # all columns as string\n",
    "trade4 = pd.read_excel(\"data/trade_data.xlsx\",dtype={'Value GBP': np.float64}) # convert value to float opposed to integer. Floats allows for decimal points\n",
    "print(trade2.dtypes,trade3.dtypes,trade4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want float for value so re-upload trade data:\n",
    "trade = pd.read_excel(\"data/trade_data.xlsx\",dtype={'Value GBP': np.float64})\n",
    "trade.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## janitor - clean_names() equivalent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with cleaner string/column names is highlgihy recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.columns = trade.columns.str.lower().str.replace(\" \",\"_\")\n",
    "trade.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using function - helpful if multiple dataframes to convert.\n",
    "def  cleanCols(df): \n",
    "    df.columns = df.columns.str.lower().str.replace(\" \",\"_\")\n",
    "    return(df)\n",
    "\n",
    "trade = cleanCols(trade)\n",
    "trade2 = cleanCols(trade2)\n",
    "trade3 = cleanCols(trade3)\n",
    "tariff = cleanCols(tariff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select columns ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = trade[[\"year\",\"flow\",\"commodity_code\",\"country_name\",\"value_gbp\"]]\n",
    "trade2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an array:\n",
    "cols = [\"year\",\"flow\",\"commodity_code\",\"country_name\",\"value_gbp\"]\n",
    "trade2 = trade[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select by column index numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trade.copy()\n",
    "# select first two columns\n",
    "df2 = df.iloc[:,0:2]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select last two columns \n",
    "df2 = df.iloc[:,5:]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first two and last two columns. \n",
    "# you can type out each individual column index. \n",
    "# But if for example you were using a large dataframe with 100s of columns and you wanted the last n and first n columns\n",
    "# It can be time consuming to type this out. you can combine both dataframes instead. \n",
    "\n",
    "df3 = pd.concat([df.iloc[:,0:2],df.iloc[:,5:]], axis = 1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 2, 4, 6 columns\n",
    "df2 = df.iloc[:,[2,4,6]]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identify column index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trade.copy()\n",
    "df.columns.get_loc(\"country_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_pos = df.columns.get_loc(\"country_name\")\n",
    "df.iloc[:,col_pos:].head(2) # select column from country_name position to end of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,:col_pos].head(2) # select start of df to index position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this didn't capture country_name - we cna add a +1\n",
    "df.iloc[:,:col_pos+1].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns\n",
    "trade2 = trade.drop([\"year\",\"flow\",\"commodity_code\"], 1) # index 1 reference columns to remove from df\n",
    "trade2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = trade.drop(cols,1)\n",
    "trade2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select columns using column indexes (numbers): tbc:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a select columns using string patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tariff data uploaded is a good df for this example as it has alot of strings with patterns which can be used for tidy selecitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tariff.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefCol = tariff.columns[tariff.columns.str.contains(pat = 'pref')]\n",
    "prefCol2 = [col for col in tariff.columns if 'pref' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prefCol,prefCol2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note difference between output types: one is an indexed array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfnCol = [col for col in tariff.columns if 'mfn' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeCol = [col for col in tariff.columns if 'commodity' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = [codeCol,mfnCol,prefCol2]\n",
    "print(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tariff2 = tariff[colNames]\n",
    "#tariff2.dtypes\n",
    "# for error fix use:\n",
    "#colNames = np.concatenate((codeCol,prefCol, mfnCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE the error.** Three list arrays have been combined together which then can't be used in this way to filter a pandas df. \n",
    "\n",
    "You can use numpy arrays for the column filters to select the data by using np.concatonate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefCol = [col for col in tariff.columns if 'pref' in col]\n",
    "mfnCol = [col for col in tariff.columns if 'mfn' in col]\n",
    "codeCol = [col for col in tariff.columns if 'commodity' in col]\n",
    "colNames2 =  np.concatenate((codeCol,prefCol, mfnCol))\n",
    "tariff2 = tariff[colNames2]\n",
    "tariff2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. select columns with numerical values and combination of string patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select columns which contain numerical values and where numerical values end the column string\n",
    "\n",
    "i.e. preferntial. + 2021, 2022 etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tariff2=tariff[[\"commodity_code\",\"preferential_applied_duty_rate_2021,\n",
    "                \"preferential_applied_duty_rate_2022\",\n",
    "                \"preferential_applied_duty_rate_2023\",\"\n",
    "                \"preferential_applied_duty_rate_2024\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were even more columns to manually type everything out is tedious and time consuming when it can easily be done using string recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = np.array(tariff.columns[tariff.columns.str.contains('.*[0-9].*', regex=True)]) # select columns with any muerical value\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doesnt create what is required - can combine str.contains multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doens't work when trying to extract numerical vlaues at end of string: (anyone know fix?)\n",
    "col_list = [col for col in tariff.columns if col.endswith('.*[0-9].*')]\n",
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternsative quick way can be a simple pattern within the numerical strings, however, extract unwanted tariff columns:\n",
    "cl = tariff.columns[tariff.columns.str.contains(pat = '20')]\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = np.array(tariff.columns[tariff.columns.str.contains('20',regex=True)]) # select columns with any muerical value\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example using startswith and endswith:\n",
    "col_list = [col for col in tariff.columns if (col.startswith('pref') & col.endswith(\"2\"))]\n",
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(tariff.columns[tariff.columns.str.contains(pat = \"pref\") & tariff.columns.str.contains('20',regex=True)])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to combine commoidty code with c in np.array\n",
    "cd = [\"commodity_code\"]\n",
    "c2 = np.concatenate((cd,c))\n",
    "tariff[c2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full solution:\n",
    "c = np.array(tariff.columns[tariff.columns.str.contains(pat = \"pref\") & tariff.columns.str.contains('20',regex=True)])\n",
    "cd = [\"commodity_code\"]\n",
    "c2 = np.concatenate((cd,c))\n",
    "tariff2 = tariff[c2]\n",
    "tariff2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Relocate columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am currnelty unaware of a single line function which acheives this like relocate in tidyverse. However it takes a few lines having specified the columns wanting to be relocated within the df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: trade data set - move flow column next to trade value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = trade.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name column(s) to be moved:\n",
    "col = trade2[\"flow\"]\n",
    "# drop column in df\n",
    "trade2.drop(labels=[\"flow\"], axis = 1, inplace = True)\n",
    "# insert column back in and select position. Value gbp is column 5(4 when index starts at 0). \n",
    "trade2.insert(4,\"flow\",col)\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can easily move multiple columns using same method:\n",
    "cols = trade2[[\"country_name\",\"country_code\"]]\n",
    "col1 = trade2[\"country_name\"]\n",
    "col2 = trade2[\"country_code\"]\n",
    "trade2.drop(cols, axis = 1, inplace = True)\n",
    "# insert column back in and select position. Value gbp is column 5(4 when index starts at 0). \n",
    "trade2.insert(1,\"country_name\",col1)\n",
    "trade2.insert(1,\"country_code\",col2)\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to move a larger selection of columns the above method isn't the most helpful. You can more easily specific the seleciton naming the order of columns (similar to select in tidyverse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = trade[[\"year\",\"country_code\",\"country_name\",\"flow\",\"commodity_code\",\"value_gbp\",\"suppression_notes\"]]\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if you have alot more columns this is also not particularly helpful if you want to decrease time writing out column names.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example df:\n",
    "    \n",
    "prefCol = [col for col in tariff.columns if 'pref' in col]\n",
    "mfnCol = [col for col in tariff.columns if 'mfn' in col]\n",
    "codeCol = [col for col in tariff.columns if 'commodity' in col]\n",
    "tariffCol = [col for col in tariff.columns if 'tariff' in col]\n",
    "colNames2 =  np.concatenate((codeCol,prefCol, mfnCol,tariffCol))\n",
    "tariff2 = tariff[colNames2]\n",
    "tariff2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alot of pattenr recogmition strings within this dataframe. However i am approaching this as if there weren't and we wanted to relocate multiple columns ot select positions within a df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tariff2 = tariff.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relocate MFN columns to front of data frame (method is useful when moving numerous columns to new position)\n",
    "cols_to_move = [\"mfn_applied_duty_rate\",\"mfn_applied_rate_ukgt\"]\n",
    "#col_index = [\"commo\n",
    "tariff3 = tariff2[cols_to_move + [ col for col in tariff2.columns if col not in cols_to_move ]]\n",
    "tariff3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tariff2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move pref columns to end of df\n",
    "cols_to_move = [col for col in tariff.columns if 'pref' in col]\n",
    "tariff3 = tariff2[[ col for col in tariff2.columns if col not in cols_to_move ]+cols_to_move]\n",
    "tariff3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Still looking for solution to move selected columns to arbitary postion in df, i,e, relocate pref columns after \"in_quota_tariff_line_code\" for example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating columns is simple in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = trade.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2[\"new_col\"] = 10\n",
    "trade2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert gbp values:\n",
    "usd = 0.8\n",
    "eur = 0.9\n",
    "trade2[\"value_usd\"] = trade2[\"value_gbp\"]*usd\n",
    "trade2[\"value_eur\"] = trade2[\"value_gbp\"]*eur\n",
    "trade2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns together\n",
    "trade2[\"new_col\"] = trade2[\"value_gbp\"]+trade2[\"value_usd\"]+trade2[\"value_eur\"]\n",
    "trade2[\"new_col2\"] = trade2[\"value_gbp\"]/100\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise column values easily:\n",
    "trade2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_gbp = trade[\"value_gbp\"].sum()\n",
    "total_value_gbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of NANs. Very useful for a quick check.\n",
    "trade.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sum across rows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: sum all new column vlaues together\n",
    "trade2[\"sum_col\"] = trade2[\"new_col\"]+trade2[\"value_usd\"]+trade2[\"value_eur\"]+trade2[\"new_col2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively name columns and sum across which is cleaner and less time to type:\n",
    "sum_cols = [\"new_col\",\"value_usd\",\"value_eur\",\"new_col2\"]\n",
    "trade2[\"sum_col2\"] = trade2[sum_cols].sum(axis=1)\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Update numerical columns only:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1:\n",
    "#numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "#for c in [c for c in trade.columns if df[c].dtype in numerics]:\n",
    "#    trade[c] = trade[c]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller one line example:\n",
    "numeric_df = trade2.apply(lambda x: x/100 if np.issubdtype(x.dtype, np.number) else x)\n",
    "numeric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update multiple columns at once:\n",
    "cols = [\"value_eur\",\"value_usd\"]\n",
    "trade2[cols] = trade2[cols]*1000\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Combine columns together:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = trade.copy()\n",
    "trade2[\"new_col\"] = trade2[\"year\"].map(str)+trade2[\"flow\"] # use map(str) as year is numeric column\n",
    "trade2[\"new_col2\"] = trade2[\"country_code\"]+\" - \"+trade2[\"country_name\"]\n",
    "trade2[\"commoidty_code2\"] = \"0\"+trade2[\"commodity_code\"]\n",
    "trade2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conditionally create columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two useful and simple ways to create and update columns using condiitonal logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade2 = trade.copy()\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**np.where**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column to indicate if value is greater than 100,000\n",
    "trade2[\"value_flag\"] = np.where(trade2[\"value_gbp\"] > 10000,\"Yes\",\"No\")\n",
    "trade2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested np.where statement:\n",
    "trade[\"value_flag\"] = np.where(trade[\"value_gbp\"] > 100000, \"100k\",\n",
    "                               np.where(trade[\"value_gbp\"] > 10000,\"10k\",\n",
    "                                        np.where(trade[\"value_gbp\"] > 1000, \"1k\",\"<1k\"))) # ensure last condition is created\n",
    "trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use & or | operaters inside np.where statements\n",
    "# create flag if country = Taiwan and value is over > 100k\n",
    "# create flag is value is > 100K or less than 1k\n",
    "\n",
    "#'' ** ensure both logical conditions are within brackets ()\n",
    "trade[\"example_flag\"] = np.where((trade[\"value_gbp\"] > 100000) & (trade[\"country_name\"] == \"Taiwan\"),\"Yes\",\"no\")\n",
    "trade[\"example_flag2\"] = np.where((trade[\"value_gbp\"] > 100000) | (trade[\"value_gbp\"] < 1000),\"Yes\",\"no\")\n",
    "trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**np.select**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.select method when dealing with multiple conditions can be help to write cleaner and more consice code to read and follow. \n",
    "\n",
    "This method you specific your conditions and outcomes within an array then define a column using this inputs within np.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [(trade[\"value_gbp\"]>100000), (trade[\"value_gbp\"] >10000), (trade[\"value_gbp\"] >1000)]\n",
    "choices = [\"100k\",\"10k\",\"1k\"]\n",
    "trade['value_flag2'] = np.select(conditions, choices, default=\"<1k\") # chnage default to 0 or any character\n",
    "trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter trade data for simple conditions like year or country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for country name = United States\n",
    "df = trade.copy()\n",
    "df = df.loc[df[\"country_name\"] == \"United States\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have ot use loc but I have grown acustomed to this method. \n",
    "df = trade.copy()\n",
    "df = df[df[\"country_name\"] == \"United States\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter using opoerators:\n",
    "# filter for United States, year and flow:\n",
    "df = trade.copy()\n",
    "df2 = df.loc[(df[\"country_name\"]==\"United States\") & (df[\"year\"] == 2020) & (df[\"flow\"] == \"Imports\")]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter if value of trade is > 10000 or less than < 1000\n",
    "df2 = df.loc[(df[\"value_gbp\"] > 10000) | (df[\"value_gbp\"] < 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** when creating multiple conditions ensure they are within brackets ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UK TRQ data set has multiple NAs throughout which will be a useful dataset to demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilise .info() for quick overview of Non-Null counts\n",
    "uk_trqs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively - quick simple sum of null values\n",
    "uk_trqs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df for NAs in geographical areas column\n",
    "na_df = uk_trqs[uk_trqs['quota__geographical_areas'].isnull()]\n",
    "na_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df for not NAs in geographical areas column\n",
    "not_na_df = uk_trqs[~(uk_trqs['quota__geographical_areas'].isnull())]\n",
    "not_na_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop NA columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any columns which has an NA value in:\n",
    "drop_na = uk_trqs.drop(uk_trqs.columns[uk_trqs.isna().sum()>len(uk_trqs.columns)],axis = 1)\n",
    "drop_na.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via using a list:\n",
    "na_cols = uk_trqs.columns[uk_trqs.isna().any()].tolist() # cretae list of columns with NAs in. \n",
    "uk_trqs2 = uk_trqs[[col for col in uk_trqs.columns if col not in na_cols]]\n",
    "uk_trqs2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns which only contain NAs i.e. (quota__monetary_unit)\n",
    "drop_na_cols = uk_trqs.dropna(axis=1, how='all') \n",
    "drop_na_cols.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using multiple OR opoerators you can use simple arrays to filter your data frame.\n",
    "\n",
    "Example - filter the trade data for Thailand, Taiwan and United States. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trade.copy()\n",
    "df2 = df.loc[(df[\"country_name\"] == \"Taiwan\") | (df[\"country_name\"] == \"Thailand\") | (df[\"country_name\"] == \"United States\")]\n",
    "pd.unique(df2[\"country_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_array = [\"Taiwan\",\"Thailand\",\"United States\"]\n",
    "df2 = df[df[\"country_name\"].isin(country_array)]\n",
    "pd.unique(df2[\"country_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is storngly preferable whne working with far greater numbers of values to filter by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_array = [\"01012100\",\"01062000\",\"02031913\",\"02031990\",\"94036090\"]\n",
    "df2 = df[df[\"commodity_code\"].isin(code_array)]\n",
    "print(pd.unique(df2[\"commodity_code\"]),df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in:\n",
    "code_array = [\"01012100\",\"01062000\",\"02031913\",\"02031990\",\"94036090\"]\n",
    "df2 = df[~(df[\"commodity_code\"].isin(code_array))]\n",
    "print(pd.unique(df2[\"commodity_code\"]),df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using other column df:\n",
    "df = trade.head(20)\n",
    "df2 = trade.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 unique codes:\n",
    "code_filt = pd.unique(df[\"commodity_code\"])\n",
    "code_filt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(df2[\"commodity_code\"]).shape)\n",
    "# 40 unique codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df2 using df will result in 20 codes:\n",
    "df3 = df2[df2[\"commodity_code\"].isin(code_filt)]\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter across columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter across columns if value exists, i.e. any vlaue column contains \"0\". tbc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tariff.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter any column in df which contains text string:\n",
    "df2 = df[df.stack().str.contains('10%').any(level=0)]\n",
    "#df2 = df[df.stack().str.contains('7%').any(level=0)]\n",
    "#df2 = df[df.stack().str.contains('Eggs').any(level=0)]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively use applymap with la,bda x and any: (will work with non-text strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.applymap(lambda x: x == \"10%\").any(1)]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter select columns rather than all df columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns to filter across. can use iloc, name columns or use string recognition:\n",
    "#1\n",
    "col_names = df.iloc[:,[5,6,7,8]]\n",
    "col_names = col_names.columns\n",
    "col_names\n",
    "#2\n",
    "col_names = [\"preferential_applied_duty_rate_2021\",\"preferential_applied_duty_rate_2022\",\"preferential_applied_duty_rate_2023\",\"preferential_applied_duty_rate_2024\"]\n",
    "#3\n",
    "col_names = np.array(tariff.columns[tariff.columns.str.contains(pat = \"pref\") & tariff.columns.str.contains('20',regex=True)])\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter across rows hwere pref tariff == x\n",
    "df2 = df[(df[col_names] == \"2%\").any(1)]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformations grouping and aggregating data is one of the most common practices I and our department does. We extract and clean large maounts of data aggregating it to more actionable outputs with teams. Groupby is essential and straight forward for aggregations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be demonstrating aggregation using the trade data set which is a very rich and useful dataset as there are multiple ways to group and summarise the data which would be useful for people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trade.copy()\n",
    "df[\"value_gbp2\"] = df[\"value_gbp\"]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by year - sum total value. Notice difference when keeping index as false:\n",
    "df_agg  = df.groupby([\"year\"])[\"value_gbp\"].sum()\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg2 = df.groupby([\"year\"], as_index = False)[\"value_gbp\"].sum()\n",
    "df_agg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by using count and mean\n",
    "df_agg = df.groupby([\"year\"])[\"value_gbp\"].count()\n",
    "# df_agg = groupby([\"year\"])[\"value_gbp\"].mean()\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple calculations of same column:\n",
    "df_agg = df.groupby([\"year\"]).agg({\"value_gbp\": [\"sum\",\"mean\",\"count\",\"max\",\"min\"]})\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple grouping for year and country\n",
    "df_agg = df.groupby([\"year\",\"flow\"]).agg({\"value_gbp\": [\"sum\",\"mean\",\"count\",\"max\",\"min\"]})\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate aggregate calculations:\n",
    "df_agg = df.groupby([\"year\",\"flow\"], as_index = False).agg({\"value_gbp\":\"sum\",\"value_gbp2\":\"mean\"})\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional aggreations (similar to sumif in excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total trade values for each year and trade flow for America:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.groupby([\"year\",\"flow\"]).apply(lambda x: x[x['country_name'] == 'United States']['value_gbp'].sum())\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way using assign and numpy:\n",
    "df_agg = df.assign(\n",
    "    us1 = np.where(df[\"country_name\"]==\"United States\",df.value_gbp,0),\n",
    "    us2 = np.where(df[\"country_name\"]==\"United States\",df.value_gbp2,0)\n",
    "   ).groupby(\"year\").agg({\"us1\":\"sum\",\"us2\":\"mean\"})\n",
    "\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is handy if you wanted to conditionally aggregate specific countries into a wider dataframe. For Example - what are the year trade values of Taiwan and Thailand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.assign(\n",
    "    thailand = np.where(df[\"country_name\"]==\"Thailand\",df.value_gbp,0),\n",
    "    taiwan = np.where(df[\"country_name\"]==\"Taiwan\",df.value_gbp,0)\n",
    "   ).groupby([\"year\",\"flow\"],as_index=False).agg({\"thailand\":\"sum\",\"taiwan\":\"sum\"})\n",
    "\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Rather than aggregated data using group by - we can grouped dataframes and calculate columns form this utilising groupby and apply. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example you may with to see the largest value as a proportion of the individual group - rather than the entire dataset. \n",
    "\n",
    "Taking the trade data set we can aggregate total trade for each country across each year. We then want to calulcate how much % each country represents for total trade across each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>value_gbp</th>\n",
       "      <th>prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>TN</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>6822944</td>\n",
       "      <td>0.00006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>TW</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>361582827</td>\n",
       "      <td>0.00331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>7982756283</td>\n",
       "      <td>0.07297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>UY</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>63050111</td>\n",
       "      <td>0.00058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>UZ</td>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>4721840369</td>\n",
       "      <td>0.04316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year country_code   country_name   value_gbp    prop\n",
       "0  2019           TN        Tunisia     6822944 0.00006\n",
       "1  2019           TW         Taiwan   361582827 0.00331\n",
       "2  2019           US  United States  7982756283 0.07297\n",
       "3  2019           UY        Uruguay    63050111 0.00058\n",
       "4  2019           UZ     Uzbekistan  4721840369 0.04316"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = trade.copy()\n",
    "df_agg = df.groupby([\"year\",\"country_code\",\"country_name\"], as_index = False).agg({\"value_gbp\":\"sum\"})\n",
    "df_agg[\"prop\"] = df_agg[\"value_gbp\"] / sum(df_agg[\"value_gbp\"]) \n",
    "df_agg.head()\n",
    "# trying this results in each countries value being divided by the entire total sum of the value column - rather than the sum of the group (year/country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>value_gbp</th>\n",
       "      <th>prop</th>\n",
       "      <th>perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>TN</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>6822944</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>TW</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>361582827</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>0.02695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>7982756283</td>\n",
       "      <td>0.07297</td>\n",
       "      <td>0.59488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>UY</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>63050111</td>\n",
       "      <td>0.00058</td>\n",
       "      <td>0.00470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>UZ</td>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>4721840369</td>\n",
       "      <td>0.04316</td>\n",
       "      <td>0.35187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019</td>\n",
       "      <td>VA</td>\n",
       "      <td>Vatican City State</td>\n",
       "      <td>251330</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019</td>\n",
       "      <td>VE</td>\n",
       "      <td>Venezuela, Bolivarian Republic of</td>\n",
       "      <td>34499624</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>0.00257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019</td>\n",
       "      <td>VI</td>\n",
       "      <td>United States Virgin Islands</td>\n",
       "      <td>462392</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019</td>\n",
       "      <td>VN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>242112223</td>\n",
       "      <td>0.00221</td>\n",
       "      <td>0.01804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019</td>\n",
       "      <td>VU</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>89558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019</td>\n",
       "      <td>YE</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>3479999</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019</td>\n",
       "      <td>ZM</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>2188358</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020</td>\n",
       "      <td>TH</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>3725874154</td>\n",
       "      <td>0.03406</td>\n",
       "      <td>0.03882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020</td>\n",
       "      <td>TJ</td>\n",
       "      <td>Tajikistan</td>\n",
       "      <td>1014619</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020</td>\n",
       "      <td>TL</td>\n",
       "      <td>Timor-Leste</td>\n",
       "      <td>565817</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020</td>\n",
       "      <td>TM</td>\n",
       "      <td>Turkmenistan</td>\n",
       "      <td>14082988</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020</td>\n",
       "      <td>TN</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>301585776</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.00314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020</td>\n",
       "      <td>TO</td>\n",
       "      <td>Tonga</td>\n",
       "      <td>472700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020</td>\n",
       "      <td>TT</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>408306612</td>\n",
       "      <td>0.00373</td>\n",
       "      <td>0.00425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020</td>\n",
       "      <td>TV</td>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>1381</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>TW</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>4108597770</td>\n",
       "      <td>0.03756</td>\n",
       "      <td>0.04281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020</td>\n",
       "      <td>TZ</td>\n",
       "      <td>Tanzania (United Republic of)</td>\n",
       "      <td>115159557</td>\n",
       "      <td>0.00105</td>\n",
       "      <td>0.00120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020</td>\n",
       "      <td>UA</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>952901530</td>\n",
       "      <td>0.00871</td>\n",
       "      <td>0.00993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020</td>\n",
       "      <td>UG</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>51902356</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>0.00054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020</td>\n",
       "      <td>UM</td>\n",
       "      <td>United States Minor outlying islands</td>\n",
       "      <td>20299</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>81920801790</td>\n",
       "      <td>0.74887</td>\n",
       "      <td>0.85358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020</td>\n",
       "      <td>UY</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>67160439</td>\n",
       "      <td>0.00061</td>\n",
       "      <td>0.00070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020</td>\n",
       "      <td>UZ</td>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>31074512</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020</td>\n",
       "      <td>VA</td>\n",
       "      <td>Vatican City State</td>\n",
       "      <td>1072256</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020</td>\n",
       "      <td>VE</td>\n",
       "      <td>Venezuela, Bolivarian Republic of</td>\n",
       "      <td>8338913</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020</td>\n",
       "      <td>VI</td>\n",
       "      <td>United States Virgin Islands</td>\n",
       "      <td>24930386</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020</td>\n",
       "      <td>VN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>4149061989</td>\n",
       "      <td>0.03793</td>\n",
       "      <td>0.04323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020</td>\n",
       "      <td>VU</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>137935</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020</td>\n",
       "      <td>WF</td>\n",
       "      <td>Wallis and Futuna</td>\n",
       "      <td>21162</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020</td>\n",
       "      <td>YE</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>23455379</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.00024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020</td>\n",
       "      <td>ZM</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>66230321</td>\n",
       "      <td>0.00061</td>\n",
       "      <td>0.00069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year country_code                          country_name    value_gbp  \\\n",
       "0   2019           TN                               Tunisia      6822944   \n",
       "1   2019           TW                                Taiwan    361582827   \n",
       "2   2019           US                         United States   7982756283   \n",
       "3   2019           UY                               Uruguay     63050111   \n",
       "4   2019           UZ                            Uzbekistan   4721840369   \n",
       "5   2019           VA                    Vatican City State       251330   \n",
       "6   2019           VE     Venezuela, Bolivarian Republic of     34499624   \n",
       "7   2019           VI          United States Virgin Islands       462392   \n",
       "8   2019           VN                               Vietnam    242112223   \n",
       "9   2019           VU                               Vanuatu        89558   \n",
       "10  2019           YE                                 Yemen      3479999   \n",
       "11  2019           ZM                                Zambia      2188358   \n",
       "12  2020           TH                              Thailand   3725874154   \n",
       "13  2020           TJ                            Tajikistan      1014619   \n",
       "14  2020           TL                           Timor-Leste       565817   \n",
       "15  2020           TM                          Turkmenistan     14082988   \n",
       "16  2020           TN                               Tunisia    301585776   \n",
       "17  2020           TO                                 Tonga       472700   \n",
       "18  2020           TT                   Trinidad and Tobago    408306612   \n",
       "19  2020           TV                                Tuvalu         1381   \n",
       "20  2020           TW                                Taiwan   4108597770   \n",
       "21  2020           TZ         Tanzania (United Republic of)    115159557   \n",
       "22  2020           UA                               Ukraine    952901530   \n",
       "23  2020           UG                                Uganda     51902356   \n",
       "24  2020           UM  United States Minor outlying islands        20299   \n",
       "25  2020           US                         United States  81920801790   \n",
       "26  2020           UY                               Uruguay     67160439   \n",
       "27  2020           UZ                            Uzbekistan     31074512   \n",
       "28  2020           VA                    Vatican City State      1072256   \n",
       "29  2020           VE     Venezuela, Bolivarian Republic of      8338913   \n",
       "30  2020           VI          United States Virgin Islands     24930386   \n",
       "31  2020           VN                               Vietnam   4149061989   \n",
       "32  2020           VU                               Vanuatu       137935   \n",
       "33  2020           WF                     Wallis and Futuna        21162   \n",
       "34  2020           YE                                 Yemen     23455379   \n",
       "35  2020           ZM                                Zambia     66230321   \n",
       "\n",
       "      prop    perc  \n",
       "0  0.00006 0.00051  \n",
       "1  0.00331 0.02695  \n",
       "2  0.07297 0.59488  \n",
       "3  0.00058 0.00470  \n",
       "4  0.04316 0.35187  \n",
       "5  0.00000 0.00002  \n",
       "6  0.00032 0.00257  \n",
       "7  0.00000 0.00003  \n",
       "8  0.00221 0.01804  \n",
       "9  0.00000 0.00001  \n",
       "10 0.00003 0.00026  \n",
       "11 0.00002 0.00016  \n",
       "12 0.03406 0.03882  \n",
       "13 0.00001 0.00001  \n",
       "14 0.00001 0.00001  \n",
       "15 0.00013 0.00015  \n",
       "16 0.00276 0.00314  \n",
       "17 0.00000 0.00000  \n",
       "18 0.00373 0.00425  \n",
       "19 0.00000 0.00000  \n",
       "20 0.03756 0.04281  \n",
       "21 0.00105 0.00120  \n",
       "22 0.00871 0.00993  \n",
       "23 0.00047 0.00054  \n",
       "24 0.00000 0.00000  \n",
       "25 0.74887 0.85358  \n",
       "26 0.00061 0.00070  \n",
       "27 0.00028 0.00032  \n",
       "28 0.00001 0.00001  \n",
       "29 0.00008 0.00009  \n",
       "30 0.00023 0.00026  \n",
       "31 0.03793 0.04323  \n",
       "32 0.00000 0.00000  \n",
       "33 0.00000 0.00000  \n",
       "34 0.00021 0.00024  \n",
       "35 0.00061 0.00069  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg[\"perc\"] = df_agg.groupby([\"year\"])[\"value_gbp\"].apply(lambda x: x/x.sum())\n",
    "df_agg\n",
    "# compare the proportion vs percentage column\n",
    "# the perc. correctly calculates the proportion of the value related to the group specified (i.e year). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnest equivilent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un-concatonate a cell broken up by delimiter into new seperate rows inside a df. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload uk_trq data with commodity codes concatoneted together in one column seperated by a delimiter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_trqs = pd.read_csv(\"data/uk_trqs.csv\",dtype={'quota__order_number': str}) # upload xlsxl\n",
    "uk_trqs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns to groupby and to unconcatonate. In this instance we have a quota level daa frame. So we select the quota order number and commodity codes. \n",
    "df = uk_trqs[[\"quota__order_number\",\"quota__commodities\"]]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE the below unnest steps won't work if NaN present in data:\n",
    "# remove NaN values. \n",
    "df = df.loc[~df[\"quota__commodities\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following steps to split out each cell within delimiters and create new row:\n",
    "new_df = pd.DataFrame(df.quota__commodities.str.split('|').tolist(), index=df.quota__order_number).stack()\n",
    "new_df = new_df.reset_index([0, 'quota__order_number'])\n",
    "new_df.columns = ['quota__order_number', 'quota__commodities']\n",
    "new_df['quota__order_number'] = new_df[ 'quota__order_number'].str.strip() # remove whitespace\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple left join using dfs with unique rows with simple one to one relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trade.groupby(\"country_name\").mean()\n",
    "df2 = trade.groupby(\"country_name\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two dataframes with same index, can join using index\n",
    "df3 = pd.merge(df,df2, left_index = True, right_index = True)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use concat with using outer join by default - using axis = 1. axis = 0 combined rows. \n",
    "df3 = pd.concat([df2,df2], axis = 1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can combine multiple dfs together using concat:\n",
    "df4 = pd.concat([df,df2,df3], axis = 1) # again note - to bind together rows chnage axis to 0. \n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge not using index:\n",
    "df = trade.groupby(\"country_name\", as_index = False).mean()\n",
    "df2 = trade.groupby(\"country_name\", as_index = False).sum()\n",
    "df3 = pd.merge(df,df2, on = \"country_name\", how = \"left\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** when different column index names, use \"left_on\" and \"right_on\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "By Default:\n",
    "\n",
    "join  is a column-wise left join\n",
    "pd.merge is a column-wise inner join\n",
    "pd.concat  is a row-wise outer join\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Other operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trade.head(20).copy()\n",
    "df2 = trade.head(20).copy()\n",
    "df3 = pd.concat((df,df2))\n",
    "# df3 dup;icated dataframe\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all rows which are duplicates will result in 20 row df:\n",
    "df_dup_remove = df3.drop_duplicates() # drop_duplicates() will remove where rows are the same across all cells. \n",
    "df_dup_remove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates based on columns. For example remvoe duplicates for \"year\" which will result in two rows.\n",
    "df_dup_remove = df3.drop_duplicates(subset = [\"year\"])\n",
    "df_dup_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trade.copy()\n",
    "df_dup_remove = df.drop_duplicates(subset = [\"flow\",\"year\"]) \n",
    "df_dup_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data based on values:\n",
    "df = trade.copy()\n",
    "df2 = df.sort_values(\"value_gbp\") # ascending = False for reverse order\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values(by=[\"year\",\"flow\",\"value_gbp\"])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe by highest traded commodity for each country for each year. \n",
    "df_sorted = df.sort_values(by=[\"country_name\",\"year\",\"value_gbp\"], ascending = False)\n",
    "df_sorted\n",
    "# note with combining this while the highest traded codes are ordered first, the alphabetical order has revered. We want the df in alphabetical order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by grouping and value by ascending - then take top two values from each country:\n",
    "df_sorted2 = df.sort_values(by=[\"country_name\",\"year\",\"value_gbp\"], ascending = False).groupby('country_name').head(2)\n",
    "df_sorted2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by grouping. If all 3 columns are within sort values and by ascedning\n",
    "# all columns are ordered by ascending when we only want the value column to be highest first. \n",
    "\n",
    "df_sorted = df.groupby([\"country_name\",\"year\"], as_index = False).apply(lambda x: x.sort_values([\"value_gbp\"], ascending = False))\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# simple uploads\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.read_excel('filepath') \n",
    "pd.read_csv('filepath')\n",
    "\n",
    "pd.read_excel('filepath', dtype={'column': str}) # convert \"column\" to string when uploading data\n",
    "pd.read_excel(\"filepath\",dtype=str) # convert all columns to string\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# simple data exploration\n",
    "\n",
    "df.dtypes # column types\n",
    "df.info # dataframe info, covers dataframe types, NaNs. \n",
    "df.shape # shape of df, i.e. number of rows, columns. \n",
    "df.describe() # summarise numerical values\n",
    "\n",
    "df.head() \n",
    "df.tail()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# clean column names\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(\" \",\"_\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Select columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# basic selection \n",
    "\n",
    "df[[\"col1\",\"col2\",\"col3\"]] # ensure double square brackets [[]]\n",
    "\n",
    "# selection using array\n",
    "\n",
    "array = [\"col1\",\"col2\",\"col3\"]\n",
    "df[array]\n",
    "\n",
    "# drop columns\n",
    "\n",
    "df.drop([\"col2\",\"col3\"], 1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a select using pattern recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "pattern_col = [col for col in df.columns if 'pattern' in col]\n",
    "pattern_col2 = [col for col in df.columns if 'pattern2' in col]\n",
    "\n",
    "# combine using np.concatonate to filter df:\n",
    "\n",
    "colNames =  np.concatenate((pattern_col, pattern_col2))\n",
    "new_df = df[colNames]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b select columns with numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "cols = np.array(df.columns[df.columns.str.contains('.*[0-9].*', regex=True)]) # select columns with any muerical value\n",
    "\n",
    "# pattern using endswith and startswith\n",
    "cols = [col for col in df.columns if col.endswith('.*[0-9].*')]\n",
    "cols = [col for col in df.columns if col.startswith('.*[0-9].*')]\n",
    "\n",
    "\n",
    "# combine numerical pattenr recongition with string\n",
    "\n",
    "col_list = [col for col in df.columns if (col.startswith('pattern') & col.endswith(\"2\"))]\n",
    "\n",
    "col_list = np.array(df.columns[df.columns.str.contains(pat = \"pref\") & df.columns.str.contains('20',regex=True)])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c relocate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "col = df[\"col1\"]\n",
    "# drop column in df\n",
    "df.drop(labels=[\"col1], axis = 1, inplace = True)\n",
    "df.insert(3,\"col1\",col) # 3 is column position (chnage to index number you want)\n",
    "\n",
    "# move multiple columns to start or end of df:\n",
    "                \n",
    "cols_to_move = [\"col1\",\"col2\",\"col3\"]\n",
    "               \n",
    "df2 = df[cols_to_move + [ col for col in df.columns if col not in cols_to_move ]]\n",
    "df2 = df[[ col for col in df.columns if col not in cols_to_move ]+cols_to_move]\n",
    "                \n",
    "              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### simple creation of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# new columns are simple to create:\n",
    "\n",
    "df[\"new_col\"] = 10 # value in all cells\n",
    "df[\"new_col\"] = df[\"value_col\"]*10\n",
    "df[\"new_col\"] = df[\"value_col1\"] + df[\"value_col2\"]\n",
    "\n",
    "# sum across rows:\n",
    "\n",
    "sum_cols = [\"col1\",\"col2\",\"col3\",\"col4\"]\n",
    "df[\"sum_col\"] = df[sum_cols].sum(axis=1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### update numerical columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# labda defined function (example columns dividing by 100):\n",
    "\n",
    "df = df.apply(lambda x: x/100 if np.issubdtype(x.dtype, np.number) else x)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### update multiple cdefined columns at once:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cols = [\"col1\",\"col2\",\"col3]\n",
    "df[cols] = df[cols]*1000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combine columns together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# equivalent to using paste in R - concatonate columns together\n",
    "\n",
    "# use \"+\"\n",
    "\n",
    "df[\"new_col\"] = df[\"value_col\"].map(str)+df[\"col2\"] # use map(str) as year is numeric column\n",
    "df[\"new_col\"] = df[\"col1\"]+\" - \"+df[\"col2\"] # create string combining two columns seperating out \"-\"\n",
    "df[\"new_col\"] = \"0\"+df[\"col1\"] # combine simple string with column\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditionally create columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### np.where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# equilvaent to R - using mutate combine with ifelse. Somethign used commonly. \n",
    "\n",
    "\n",
    "# np.where\n",
    "\n",
    "df[\"new_col\"] = np.where(df[\"value_col\"] > 10000,\"Yes\",\"No\")\n",
    "\n",
    "# nested np.where statement:\n",
    "\n",
    "df[\"value_flag\"] = np.where(df[\"value_col\"] > 100000, \"100k\",\n",
    "                               np.where(df[\"value_col\"] > 10000,\"10k\",\n",
    "                                        np.where(df[\"value_col\"] > 1000, \"1k\",\"<1k\")))\n",
    "\n",
    "# using logicial operaters: \n",
    "# ensure each condition is inside a bracket ()\n",
    "df[\"example_flag\"] = np.where((df[\"value_col\"] > 100000) & (df[\"col\"] == \"Taiwan\"),\"Yes\",\"no\")\n",
    "df[\"example_flag2\"] = np.where((df[\"value_col\"] > 100000) | (df[\"value_col\"] < 1000),\"Yes\",\"no\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### np.select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.select is very useful for writing more concise and clean code when multiple conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conditions = [(df[\"value_col\"]>100000), (df[\"value_col\"] >10000), (df[\"value_col\"] >1000)]\n",
    "choices = [\"100k\",\"10k\",\"1k\"]\n",
    "df['value_col'] = np.select(conditions, choices, default=\"<1k\") # chnage default to 0 or any character\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### basic filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "\n",
    "there are multiple ways to filter a dataframe. The only two I use for simple filtering are:\n",
    "    \n",
    "df = df[df[\"col\"] == \"condition\"]\n",
    "df = df.loc[df[\"col\"] == \"condition]\n",
    "            \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "filter using operators (ensure use of brackets () )\n",
    "    \n",
    "df = df.loc[(df[\"col1\"] == \"condition\") & (df[\"col2\"] == \"condition2\")]\n",
    "\n",
    "df = df.loc[(df[\"val1\"] > 100) | (df[\"val2\"] < 10)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# filter where col is NaN\n",
    "na_df = df[df[\"col\"].isnull()]\n",
    "\n",
    "# filter where col is NOT NaN\n",
    "\n",
    "not_na_df = df[~(df[\"col\"].isnull())]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "drop_na = df.drop(df.columns[df.isna().sum()>len(df.columns)],axis = 1)\n",
    "\n",
    "# OR\n",
    "\n",
    "na_cols = df.columns[df.isna().any()].tolist() # cretae list of columns with NAs in. \n",
    "drop_na = df[[col for col in df.columns if col not in na_cols]]\n",
    "\n",
    "\n",
    "# drop columns which only contain NAs \n",
    "drop_na_cols = df.dropna(axis=1, how='all') \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter by arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "array = [\"value1\",\"value2\",\"value3\"]\n",
    "df2 = df[df[\"value_col\"].isin(array)]\n",
    "\n",
    "# not in\n",
    "array = [\"value1\",\"value2\",\"value3\"]\n",
    "df2 = df[~(df[\"value_col\"].isin(array))]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter across columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# filter any columns which contains string:\n",
    "\n",
    "df = df[df.stack().str.contains('string').any(level=0)]\n",
    "\n",
    "# OR\n",
    "\n",
    "df = df[df.applymap(lambda x: x == \"string\").any(1)]\n",
    "\n",
    "# filter across selected columns:\n",
    "\n",
    "columns_to_filt = [\"col1\", \"col2\", \"col3\", \"col4\"]\n",
    "\n",
    "df2 = df[(df[columns_to_filt] == \"condition\").any(1)] \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### basic aggregations using groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# basic and quick aggregation:\n",
    "\n",
    "df.groupby([\"col_agg\"])[\"col_value\"].sum # (.count, .mean etc)\n",
    "df.groupby([\"col_agg\"], as_index = False)[\"col_value\"].sum # use as_index = False to remove index and have as a column\n",
    "\n",
    "# multiple calculations one one value\n",
    "\n",
    "df.groupby([\"col_agg\"]).agg({\"col_value\": [\"sum\",\"mean\",\"count\",\"max\",\"min\"]})\n",
    "\n",
    "# multiple conditions within aggregation:\n",
    "\n",
    "df.groupby([\"col_agg1\",\"col_agg2\",\"col_agg3\"]).agg({\"col_value\": \"sum\"})\n",
    "\n",
    "# seperate aggregate calculations:\n",
    "df.groupby([\"col_agg1\",\"col_agg2\"], as_index = False).agg({\"col_value1\":\"sum\",\"col_value2\":\"mean\"})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grouped calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ensure first grouping\n",
    "df_agg = df.groupby([\"col1\",\"col2\"].agg({\"value\":\"sum\"})\n",
    "                    \n",
    "# grouped calculation\n",
    "df_agg[\"perc\"] = df_agg.groupby([\"col1\"])[\"value\"].apply(lambda x: x/x.sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### conditional aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# equivalent to sumif\n",
    "df.groupby([\"col_agg\").apply(lambda x: x[x['col'] == 'condition']['col_value'].sum())\n",
    "            \n",
    "# alternative way using assign and numpy:\n",
    "            \n",
    "df_agg = df.assign(\n",
    "    val1 = np.where(df[\"col\"]==\"condition\",df.col_value1,0),\n",
    "    val2 = np.where(df[\"col\"]==\"condition\",df.col_value2,0)\n",
    "   ).groupby(\"col_agg\").agg({\"val1\":\"sum\",\"val2\":\"mean\"})\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "df3 = pd.merge(df,df2, left_index = True, right_index = True)\n",
    "\n",
    "# merge using defined \"how\"\n",
    "df3 = pd.merge(df,df2,on = \"joinID\", how = \"left\") # can be placed by right, inner etc. \n",
    "\n",
    "# concat (default outer join if index is 1, 0 for row bind\n",
    "\n",
    "df4 = pd.concat([df,df2,df3], axis = 1) \n",
    "df4 = pd.concat([df,df2,df3], axis = 0) \n",
    "\n",
    "\n",
    "By Default:\n",
    "\n",
    "join  is a column-wise left join\n",
    "pd.merge is a column-wise inner join\n",
    "pd.concat  is a row-wise outer join\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Other operators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.drop_duplicates()\n",
    "df.drop_duplicates(subset=[\"col1\",\"col2\"])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "df.sort_values(by=[\"col1\"])\n",
    "df.sort_values(by=[\"col1\"], ascending = False) # highest value first\n",
    "df.sort_values(by=[\"col1\",\"col2\",\"col3\"]) # multiple grouping \n",
    "\n",
    "# sort grouped data\n",
    "df.groupby([\"group1\",\"group2\"]).apply(lambda x: x.sort_values([\"sort_column\"], ascending = False))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
